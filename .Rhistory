}
setwd("/Users/akterminsprove/Desktop/DTU/5. Semester/02418 Statistisk Modellering Teori og Anvendelser/Assignments/Financial Data")
#setwd("/Users/akterminsprove/Desktop/DTU/5. Semester/02418 Statistisk Modellering Teori og Anvendelser/Assignments/Financial Data")
data <- read.csv("finance_data.csv", header=TRUE,sep=";")
data$time=as.Date(data$time)
head(data,6)
plot(data$time,data$SLV)
qqnorm(data$SLV, pch = 1, frame = FALSE)
qqline(data$SLV, col = "steelblue", lwd = 2)
normal.function=function(dat){
normal.likelihood=function(params,dat){-sum(dnorm(x=dat,params[1],params[2],log=TRUE))}
#Find the optimal parameters using nlimnb:
opt=nlminb(c(0.001,0.001),normal.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(normal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2, "se"=se))
}
normal.function(data$SLV)
x=data$SLV
h<-hist(x, xlab="SLV",
main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,normal.function(data$SLV)$opt$par[1],normal.function(data$SLV)$opt$par[2])
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
library(metRology)
install.packages("metRology")
#setwd("/Users/akterminsprove/Desktop/DTU/5. Semester/02418 Statistisk Modellering Teori og Anvendelser/Assignments/Financial Data")
data <- read.csv("finance_data.csv", header=TRUE,sep=";")
data$time=as.Date(data$time)
head(data,6)
plot(data$time,data$SLV)
qqnorm(data$SLV, pch = 1, frame = FALSE)
qqline(data$SLV, col = "steelblue", lwd = 2)
normal.function=function(dat){
normal.likelihood=function(params,dat){-sum(dnorm(x=dat,params[1],params[2],log=TRUE))}
#Find the optimal parameters using nlimnb:
opt=nlminb(c(0.001,0.001),normal.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(normal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2, "se"=se))
}
normal.function(data$SLV)
x=data$SLV
h<-hist(x, xlab="SLV",
main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,normal.function(data$SLV)$opt$par[1],normal.function(data$SLV)$opt$par[2])
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
library(metRology)
t.function=function(dat){
t.likelihood=function(params,dat){
-sum(dt.scaled(x=dat, df=params[1], mean = params[2], sd = params[3], log = TRUE))}
#Find the optimal parameters using nlimnb:
opt=nlminb(c(1,0.001,0.001),t.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(t.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
CI3=opt$par[3]+c(-1,1)*se[3]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2,"CI3"=CI3, "se"=se))
}
t.function(data$SLV)
x=data$SLV
h<-hist(x, xlab="SLV",
main="Histogram with t-curve Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dt.scaled(xfit, df=t.function(data$SLV)$opt$par[1],mean = t.function(data$SLV)$opt$par[2],sd =t.function(data$SLV)$opt$par[3])
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
AIC=function(nll,k){
2*k-2*(-nll)}
library(kableExtra)
results =matrix(NA,ncol=11, nrow=2)
colnames(results) = c("","AIC", "DF", "Mean", "Sd", "CI lower DF","CI upper DF","CI lower Mean","CI upper Mean","CI lower Sd","CI upper Sd")
results[1:2,1]=c("Normal Distribution", "t - Distribution")
results[1,2] =round(AIC(normal.function(data$SLV)$opt$objective,2))
results[1,3] =NA
results[1,4] =round(normal.function(data$SLV)$opt$par[1],4)
results[1,5] =round(normal.function(data$SLV)$opt$par[2],4)
results[1,6] = NA
results[1,7] =NA
results[1,8] =round(normal.function(data$SLV)$CI1[1],4)
results[1,9] =round(normal.function(data$SLV)$CI1[2],4)
results[1,10] = round(normal.function(data$SLV)$CI2[1],4)
results[1,11] = round(normal.function(data$SLV)$CI2[2],4)
results[2,2] =round(AIC(t.function(data$SLV)$opt$objective,2))
results[2,3] =round(t.function(data$SLV)$opt$par[1],4)
results[2,4] =round(t.function(data$SLV)$opt$par[2],4)
results[2,5] =round(t.function(data$SLV)$opt$par[3],4)
results[2,6] =round(t.function(data$SLV)$CI1[1],4)
results[2,7] =round(t.function(data$SLV)$CI1[2],4)
results[2,8] =round(t.function(data$SLV)$CI2[1],4)
results[2,9] =round(t.function(data$SLV)$CI2[2],4)
results[2,10] = round(t.function(data$SLV)$CI3[1],4)
results[2,11] = round(t.function(data$SLV)$CI3[2],4)
results %>%kbl() %>%kable_styling(full_width = TRUE)
?dt.scaled
data <- read.delim("Logistic.txt",header = TRUE, sep = "", dec = ".")
data$AIDS_no=data$n-data$AIDS_yes
head(data,2)
n <- sum(data[,3]) #Number of patient #n
k <- sum(data[,2]) #Number of patients with AIDS #Sucesses #k
theta <- k/n #Probability that you have Aids
#Formulating the likelihood function
binomial.log.likelihood = function(theta,success,size){
-(sum(dbinom(x=success,size=size,prob=theta, log=TRUE)))}
#(theta.opt=optimize(binomial.log.likelihood, c(0.01,0.99),success=sum(data$AIDS_yes),size=sum(data$n))$minimum)
(nlminb(start=0.5,objective=binomial.log.likelihood,lower=0,success=k,size=n)$par)
#Theta for patients treated with AZT. This is the probability of having AIDS when treated with AZT.
#(theta_AZT_yes.opt=optimize(binomial.log.likelihood, c(0.01,0.99),x=data$AIDS_yes[1],size=data$n[1])$minimum)
(theta_AZT_yes.opt=nlminb(start=0.5,binomial.log.likelihood,lower=0,success=data$AIDS_yes[1],size=data$n[1])$par)
(theta_AZT_no.opt=nlminb(start=0.5,binomial.log.likelihood,lower=0,success=data$AIDS_yes[2],size=data$n[2])$par)
#Theta for patients treated without AZT. This is the probability of having AIDS without treament.
#(theta_AZT_no.opt=optimize(binomial.log.likelihood, c(0.01,0.99),x=data$AIDS_yes[2],size=data$n[2])$minimum)
k1 <- data$AIDS_yes[1] #ATZ_yes
k2 <- data$AIDS_yes[2] #ATZ_no
n1 <- data$n[1]
n2 <- data$n[2]
p1 <- k1/n1
p2 <- k2/n2
p <- k/n
library(numDeriv)
I <- hessian(binomial.log.likelihood,theta,success=k,size=n)
I_AZT_yes <- hessian(binomial.log.likelihood,theta_AZT_yes.opt,success=k1,size=n1)
I_AZT_no <- hessian(binomial.log.likelihood,theta_AZT_no.opt,success=k2,size=n2)
#The standard error
se <- 1/sqrt(I)
se_AZT_yes <- 1/sqrt(I_AZT_yes)
se_AZT_no <- 1/sqrt(I_AZT_no)
#confidence intervallet
(p+c(-1,1)*qnorm(0.975)*se/sqrt(n))
(p1+c(-1,1)*qnorm(0.975)*se_AZT_yes/sqrt(n1))
(p2+c(-1,1)*qnorm(0.975)*se_AZT_no/sqrt(n2))
x=data$AIDS_yes[1]
y=data$AIDS_no[1]
m=sum(data$AIDS_yes)
n=sum(data$AIDS_no)
N=m+n
t=x+y
#The standard test of equality of proportions is the famous chi-squared test
(chi.squared.test=N*(x*(n-y)-y*(m-x))^2/(m*n*t*(N-t)))
(pchisq(chi.squared.test, df=1, lower.tail=FALSE))
library(magrittr)
log_data = data
#Creating likelihood function for b0 and b1
logL_partial <- function(b0, b1, data, AZT = TRUE){
y = if (AZT) {data[1,2]} else {data[2,2]} %>% as.numeric()
n = if (AZT) {data[1,3]} else {data[2,3]} %>% as.numeric()
false = n-y
y*((b0 + b1 * AZT) * 1 - log(1+exp(b0+b1*AZT))) + false*((b0 + b1 * AZT) * 0 - log(1+exp(b0+b1*AZT)))
}
logL <- function(theta, data){
- (logL_partial(theta[1], theta[2],data, F) + logL_partial(theta[1],theta[2],data, T))
}
# Finding the optimal values for b0 and b1
opt <- nlminb(c(0, 0), logL, lower = c(-Inf,-Inf), data = log_data)
#Calculate p0 and p1
p0 <- exp(opt$par[1])/(1+exp(opt$par[1]))
p1 <- exp(opt$par[1] + opt$par[2])/(1+exp(opt$par[1] + opt$par[2]))
# Creating profile likelihood for b1
logL_b1 <- function(theta, b1, data){
-(logL_partial(theta[1], b1,data, F) + logL_partial(theta[1],b1,data, T))
}
profile_b1 <- function(b1, data){
opt <- nlminb(c(0), logL_b1, lower = c(-Inf), data = log_data, b1 = b1)
# optimize(logL)
b0 <- opt$par
-logL_b1(c(b0),b1,data)
}
b1 <- seq(-5,5,0.01)
alpha <- 0.05
c <- exp(-1/2*qchisq(1-alpha,1))
p = c()
for (i in 1:length(b1)){
p[i] = exp(profile_b1(b1[i], log_data))
}
MLE_b1 <- optimize(profile_b1, c(-5,5), data = log_data, maximum = T)$maximum
plot(b1, p/max(p), 'l')
abline(c, 0)
abline(v = MLE_b1)
a_b1 <- b1[p/max(p) > c]
CI_b1 <- c(min(a_b1), max(a_b1))
cat("The maximum likelihood estimates of p0 and p1 are: p0 = ", p0, " p1 = ", p1, "\n")
cat("The maximum profile likelihood estimate of b1 is:  ", MLE_b1, "\n")
cat("With confidence intervals: ", CI_b1, "\n")
(opt$par)
#Load data
data.time <- read.delim("actg320.txt",header = TRUE, sep = "", dec = ".")
head(data.time,2)
#Count number of patient without treatment and without AIDS
(sum(data.time$tx == 0 & data.time$event == 0))
#Count number of patient without treatment and AIDS
(sum(data.time$tx == 0 & data.time$event == 1))
#Count number of patient with treatment and without AIDS
(sum(data.time$tx == 1 & data.time$event == 0))
# Count number of patient with treatment and AIDS
(sum(data.time$tx == 1 & data.time$event == 1))
#Calculating proportions
#No aids out of people without treatment
(sum(data.time$tx == 0 & data.time$event == 0))/sum(data.time$tx == 0)
#Aids out of people without treatment
(sum(data.time$tx == 0 & data.time$event == 1))/sum(data.time$tx == 0)
#No Aids out of people with treatment
(sum(data.time$tx == 1 & data.time$event == 0))/sum(data.time$tx == 1)
#Aids out of people with treatment
(sum(data.time$tx == 1 & data.time$event == 1))/sum(data.time$tx == 1)
# Likelihood function for all patients with treat and without treatment. (event=1 is treatment). (pxep is used to define the probability after a certain point in time)
exp.likelihood=function(rate){
-sum(dexp(x=data.time$time[data.time$event==1], rate=rate, log=TRUE))- sum(pexp(q=data.time$time[data.time$event==0], lower.tail = FALSE, rate=rate, log=TRUE))
}
(nlminb(start=0.5,objective = exp.likelihood,lower=0)$par)
(nlminb(start=0.5,objective = exp.likelihood,lower=0)$objective)
#(optimize(exp.likelihood,c(0, 100),maximum=TRUE)$maximum)
# Likelihood function for no treatment group. (p = cumulative function, d=density function)
exp.likelihood_no_treat=function(rate){
-sum(dexp(x=data.time$time[data.time$event==1 & data.time$tx == 0], rate=rate, log=TRUE))- sum(pexp(q=data.time$time[data.time$event==0 & data.time$tx == 0], lower.tail = FALSE, rate=rate, log=TRUE))
}
(nlminb(start=0.5,objective = exp.likelihood_no_treat,lower=0)$par)
(nlminb(start=0.5,objective = exp.likelihood_no_treat,lower=0)$objective)
# Likelihood function for treatment group
exp.likelihood_with_treat=function(rate){
-sum(dexp(x=data.time$time[data.time$event==1 & data.time$tx == 1], rate=rate, log=TRUE))- sum(pexp(q=data.time$time[data.time$event==0 & data.time$tx == 1], lower.tail = FALSE, rate=rate, log=TRUE))
}
(nlminb(start=0.5,objective = exp.likelihood_with_treat,lower=0)$par)
(nlminb(start=0.5,objective = exp.likelihood_with_treat,lower=0)$objective )
# Implement negative log likelihood function
nll.exp <- function(rate){
# Part for people with Aids
val1 <- -sum(dexp(x=data.time$time[data.time$event==1],rate=rate[1]+rate[2]*data.time$tx[data.time$event==1],log=TRUE))
# Part for people without Aids
val2 <- -sum(pexp(q=data.time$time[data.time$event==0],rate=rate[1]+rate[2]*data.time$tx[data.time$event==0],lower.tail=F,log=TRUE))
val1+val2
}
# Find optimal rate:
opt.exp <- nlminb(start=c(0.001,0.005),objective=nll.exp,lower=c(-Inf,-Inf))
# b1 -  is the treatment effect
(opt.exp$par[2])
#Making a confidence interval
HE <- hessian(nll.exp,opt.exp$par)
se <- sqrt(diag(solve(HE)))
#Confidence interval for b1
(opt.exp$par[2]+c(-1,1)*qnorm(0.975)*se[2])
nll.b1 <- function(rate,b1){
val1 <- -sum(dexp(x=data.time$time[data.time$event==1],rate=rate[1]+b1*data.time$tx[data.time$event==1],log=TRUE))
val2 <- -sum(pexp(q=data.time$time[data.time$event==0],rate=rate[1]+b1*data.time$tx[data.time$event==0],lower.tail=F,log=TRUE))
val1+val2
}
profile.b1 <- function(b1){
opt <- nlminb(start=c(0.001),objective=nll.exp,lower=c(-Inf),b1=b1)
b0 <-opt$par
-nll.b1(c(b0),b1)
}
b1 <- seq(-0.0005,0.0005,0.00001)
n_b1 = length(b1)
p = rep(NaN,n_b1)
# for (i in 1:length(b1)) {
#    p[i] = exp(profile.b1(b1[i]))
#  }
#
# plot(b1,p/max(p),'l')
n_b1 = length(b1)
profile.b1.plot = rep(NaN,n_b1)
for (i in 1:length(b1)){
profile.b1.plot[i] = profile.b1(b1[i])
}
? ggpairs
?ggparis
?ggpairs
data <- read.delim("tuno.txt",header = TRUE, sep = " ", dec = ".")
head(data,6)
summary(data[c(0,4:6)],digits = 2)
library("ggplot2")                     # Load ggplot2 package
library("GGally")
ggpairs(data[c(0,4:6)])
normalize <- function(x) {
return ((x+0.01 - min(x)) / (max(x) - min(x)+0.04))}
data$pow.obs.norm<-normalize(data$pow.obs)
data$ws30.norm<-normalize(data$ws30)
data$wd30.norm<-normalize(data$wd30)
head(data,6)
# Find the Fisher information:
H=hessian(Beta.likelihood,opt$par, dat=dat)
library(numDeriv)
beta.function = function(dat){
# Define likelihood function:
Beta.likelihood=function(params,dat){ -sum(dbeta(x=dat,shape1=params[1], shape2=params[2], log=TRUE))}
# Find the optimal parameters using nlimnb:
opt=nlminb(c(0.01,0.01),Beta.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(Beta.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2, "se"=se))
}
library(numDeriv)
beta.function = function(dat){
# Define likelihood function:
Beta.likelihood=function(params,dat){ -sum(dbeta(x=dat,shape1=params[1], shape2=params[2], log=TRUE))}
# Find the optimal parameters using nlimnb:
opt=nlminb(c(0.01,0.01),Beta.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(Beta.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2, "se"=se))
}
gamma.function=function(dat){
# Define likelihood function:
Gamma.likelihood=function(params, dat){ -sum(dgamma(x=dat,shape=params[1], rate=params[2], log=TRUE))}
# Find the optimal parameters using nlimnb:
opt=nlminb(c(0.01,0.01),Gamma.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(Gamma.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2, "se"=se))
}
log.normal.function=function(dat){
# Define likelihood function:
LogNormal.likelihood=function(params,dat){ -sum(dlnorm(x=dat,meanlog=params[1], sdlog=params[2], log=TRUE))}
# Find the optimal parameters using nlimnb:
opt=nlminb(c(0.01,0.01),LogNormal.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(LogNormal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2, "se"=se))
}
log.normal.function=function(dat){
# Define likelihood function:
LogNormal.likelihood=function(params,dat){ -sum(dlnorm(x=dat,meanlog=params[1], sdlog=params[2], log=TRUE))}
# Find the optimal parameters using nlimnb:
opt=nlminb(c(0.01,0.01),LogNormal.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(LogNormal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2, "se"=se))
}
library(MASS)
#boxcox.lambda=boxcox(lm(data$pow.obs.norm~1),lambda=lambda)
## box-cox transformation
bc.trans <- function(lambda,y){
y.l <- (y^lambda-1)/lambda
if(lambda==0){y.l <- log(y)}
return(y.l)}
## profile likelihood for lambda
lp.lambda <- function(lambda,y){
n <- length(y)
y.l <- bc.trans(lambda ,y)
sigmasq <- 1/n * sum((y.l-mean(y.l))^2)
-n/2 * log(sigmasq) + (lambda-1)*sum(log(y))}
(opt.lambda.boxcox=optimize(lp.lambda,c(-2,2),y=data$pow.obs,maximum=TRUE))
#setwd("/Users/akterminsprove/Desktop/DTU/5. Semester/02418 Statistisk Modellering Teori og Anvendelser/Assignments/Financial Data")
data <- read.csv("finance_data.csv", header=TRUE,sep=";")
data$time=as.Date(data$time)
head(data,6)
library(metRology)
t.function=function(dat){
t.likelihood=function(params,dat){
-sum(dt.scaled(x=dat, df=params[1], mean = params[2], sd = params[3], log = TRUE))}
#Find the optimal parameters using nlimnb:
opt=nlminb(c(1,0.001,0.001),t.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(t.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
CI3=opt$par[3]+c(-1,1)*se[3]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2,"CI3"=CI3, "se"=se))
}
t.function(data$SLV)
AIC=function(nll,k){
2*k-2*(-nll)}
library(kableExtra)
results =matrix(NA,ncol=11, nrow=2)
colnames(results) = c("","AIC", "DF", "Mean", "Sd", "CI lower DF","CI upper DF","CI lower Mean","CI upper Mean","CI lower Sd","CI upper Sd")
results[1:2,1]=c("Normal Distribution", "t - Distribution")
results[1,2] =round(AIC(normal.function(data$SLV)$opt$objective,2))
library(kableExtra)
results =matrix(NA,ncol=11, nrow=2)
colnames(results) = c("","AIC", "DF", "Mean", "Sd", "CI lower DF","CI upper DF","CI lower Mean","CI upper Mean","CI lower Sd","CI upper Sd")
results[1:2,1]=c("Normal Distribution", "t - Distribution")
results[1,2] =round(AIC(normal.function(data$SLV)$opt$objective,2))
#setwd("/Users/akterminsprove/Desktop/DTU/5. Semester/02418 Statistisk Modellering Teori og Anvendelser/Assignments/Financial Data")
data <- read.csv("finance_data.csv", header=TRUE,sep=";")
data$time=as.Date(data$time)
head(data,6)
plot(data$time,data$SLV)
qqnorm(data$SLV, pch = 1, frame = FALSE)
qqline(data$SLV, col = "steelblue", lwd = 2)
normal.function=function(dat){
normal.likelihood=function(params,dat){-sum(dnorm(x=dat,params[1],params[2],log=TRUE))}
#Find the optimal parameters using nlimnb:
opt=nlminb(c(0.001,0.001),normal.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(normal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2, "se"=se))
}
normal.function(data$SLV)
#setwd("/Users/akterminsprove/Desktop/DTU/5. Semester/02418 Statistisk Modellering Teori og Anvendelser/Assignments/Financial Data")
data <- read.csv("finance_data.csv", header=TRUE,sep=";")
data$time=as.Date(data$time)
head(data,6)
plot(data$time,data$SLV)
qqnorm(data$SLV, pch = 1, frame = FALSE)
qqline(data$SLV, col = "steelblue", lwd = 2)
normal.function=function(dat){
normal.likelihood=function(params,dat){-sum(dnorm(x=dat,params[1],params[2],log=TRUE))}
#Find the optimal parameters using nlimnb:
opt=nlminb(c(0.001,0.001),normal.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(normal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2, "se"=se))
}
normal.function(data$SLV)
library(numDeriv)
#setwd("/Users/akterminsprove/Desktop/DTU/5. Semester/02418 Statistisk Modellering Teori og Anvendelser/Assignments/Financial Data")
data <- read.csv("finance_data.csv", header=TRUE,sep=";")
data$time=as.Date(data$time)
head(data,6)
plot(data$time,data$SLV)
qqnorm(data$SLV, pch = 1, frame = FALSE)
qqline(data$SLV, col = "steelblue", lwd = 2)
normal.function=function(dat){
normal.likelihood=function(params,dat){-sum(dnorm(x=dat,params[1],params[2],log=TRUE))}
#Find the optimal parameters using nlimnb:
opt=nlminb(c(0.001,0.001),normal.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(normal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2, "se"=se))
}
normal.function(data$SLV)
x=data$SLV
h<-hist(x, xlab="SLV",
main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,normal.function(data$SLV)$opt$par[1],normal.function(data$SLV)$opt$par[2])
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
library(metRology)
t.function=function(dat){
t.likelihood=function(params,dat){
-sum(dt.scaled(x=dat, df=params[1], mean = params[2], sd = params[3], log = TRUE))}
#Find the optimal parameters using nlimnb:
opt=nlminb(c(1,0.001,0.001),t.likelihood, dat=dat)
# Find the Fisher information:
H=hessian(t.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))
CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)
CI3=opt$par[3]+c(-1,1)*se[3]*qnorm(0.975)
return(list("opt"=opt,"CI1"=CI1, "CI2"=CI2,"CI3"=CI3, "se"=se))
}
t.function(data$SLV)
x=data$SLV
h<-hist(x, xlab="SLV",
main="Histogram with t-curve Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dt.scaled(xfit, df=t.function(data$SLV)$opt$par[1],mean = t.function(data$SLV)$opt$par[2],sd =t.function(data$SLV)$opt$par[3])
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
AIC=function(nll,k){
2*k-2*(-nll)}
library(kableExtra)
results =matrix(NA,ncol=11, nrow=2)
colnames(results) = c("","AIC", "DF", "Mean", "Sd", "CI lower DF","CI upper DF","CI lower Mean","CI upper Mean","CI lower Sd","CI upper Sd")
results[1:2,1]=c("Normal Distribution", "t - Distribution")
results[1,2] =round(AIC(normal.function(data$SLV)$opt$objective,2))
results[1,3] =NA
results[1,4] =round(normal.function(data$SLV)$opt$par[1],4)
results[1,5] =round(normal.function(data$SLV)$opt$par[2],4)
results[1,6] = NA
results[1,7] =NA
results[1,8] =round(normal.function(data$SLV)$CI1[1],4)
results[1,9] =round(normal.function(data$SLV)$CI1[2],4)
results[1,10] = round(normal.function(data$SLV)$CI2[1],4)
results[1,11] = round(normal.function(data$SLV)$CI2[2],4)
results[2,2] =round(AIC(t.function(data$SLV)$opt$objective,2))
results[2,3] =round(t.function(data$SLV)$opt$par[1],4)
results[2,4] =round(t.function(data$SLV)$opt$par[2],4)
results[2,5] =round(t.function(data$SLV)$opt$par[3],4)
results[2,6] =round(t.function(data$SLV)$CI1[1],4)
results[2,7] =round(t.function(data$SLV)$CI1[2],4)
results[2,8] =round(t.function(data$SLV)$CI2[1],4)
results[2,9] =round(t.function(data$SLV)$CI2[2],4)
results[2,10] = round(t.function(data$SLV)$CI3[1],4)
results[2,11] = round(t.function(data$SLV)$CI3[2],4)
results %>%kbl() %>%kable_styling(full_width = TRUE)
