---
title: "Wind Part 3"
output: pdf_document
---

## Read the data tuno.txt into R.
The data is read, and the first few lines of the code is shown.
```{r}
data <- read.delim("tuno.txt",header = TRUE, sep = " ", dec = ".")
data$pow.obs.norm<-(data$pow.obs)/5000
head(data,6)
```
```{r}
plot(data$r.day,data$pow.obs.norm,type='o')
```

## Fit optimal linear model

```{r}
trans1 <- function(lambda,y){
    y.l <- 1/lambda*log((y^lambda)/(1-y^lambda))
    return(y.l)}

## profile likelihood for lambda
lp.lambda1 <- function(lambda,y){
    mod <- lm(trans1(lambda,y)~data$ws30+I(data$ws30^2)+cos(data$wd30))
    length(y)/2*log(summary(mod)$sigma^2) - sum(log(abs(1/(y*(1 - y^lambda)))))
    }

(opt.lambda.trans1=optimize(lp.lambda1,c(0,1),y=data$pow.obs.norm))

mod0=glm(trans1(opt.lambda.trans1$minimum,data$pow.obs.norm)~data$ws30+I(data$ws30^2)+cos(data$wd30),family=gaussian,  data=data)

mod0
```


## 1. Extract residuals and construct matrix(e)
```{r}

e <- cbind(mod0$residuals[1:length(mod0$residuals)-1],mod0$residuals[2:length(mod0$residuals)])

# e <- cbind(mod0$residuals[2:length(mod0$residuals)],mod0$residuals[1:length(mod0$residuals)-1])

var(e);cor(e)

```

## 2. Fit e to a normal distribution
### Parameter estimates and wald confidence intervals
For model where $\rho$ is not equal to zero.
```{r}
library(numDeriv)
library(mvtnorm)
normal.function=function(dat){
# Define likelihood function:
Normal.likelihood=function(params,dat){ -sum(dmvnorm(e, mean=c(0,0),sigma=params[1]*matrix(c(1,params[2], params[2],1), nrow=2),log=TRUE))}

# Find the optimal parameters using nlimnb:
opt=nlminb(c(0.01,0.01),Normal.likelihood,lower=c(0,-1),upper=c(Inf, 1), dat=dat)
# Find the Fisher information:
H=hessian(Normal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))

CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)

return(list("opt"=opt,"CI1"=CI1,"CI2"=CI2, "se"=se, "H"=H))
}

mod2<-normal.function(e)
mod2
```


### A contour plot of the likelihood

```{r}

sig1 = seq(9,15,length.out = 100)
rho1 = seq(0,0.7, length.out = 100)
par(mfrow=c(1,1))
ll.contour = function(sigma,rho, e){
  n <- length(e) - 1
  #print(co)
  tmp=0
  deter = sigma^2-rho^2*sigma^2
  for (i in 1:n){
    x=e[i]; y=e[i+1]
    tmp = tmp + 1/2*( (-x/( sigma*(rho^2 - 1)) + y*rho/( sigma*(rho^2 - 1)))*x 
                                               + (x*rho/( sigma*(rho^2 - 1)) - y/( sigma*(rho^2 - 1)))*y)
  }
  -2/2*log(2*pi)*n-1/2*log(deter)*n - tmp #1/(2*sigma) * sum((y[-1]-co*y[-(n+1)])^2)
}
ll<- outer(sig1,rho1,'ll.contour',e=e)
like<- exp(ll-max(ll))
contour(sig1,rho1,like,level=c(0.05,.1,.3,.5,.7,.9),
        ylab=expression(rho),
        xlab=expression(sigma^2))
title(expression('Likelihood contour'))
#lines(c(0,50),c(0,0),lty=2,col=2)
abline(v = 11.18662, lty = 2, col = 'red')
abline(v = 14.27960, lty = 2, col = 'red')
abline(h = 0.2156042, lty = 2, col = 'red')
abline(h = 0.4233721, lty = 2, col = 'red')

```


### P-values for the likelihood ratio test
Null model, where $\rho=0$.

```{r}
normal.function=function(dat){
# Define likelihood function:
Normal.likelihood=function(params,dat){ -sum(dmvnorm(e, mean=c(0,0),sigma=params[1]*matrix(c(1,0,0,1), nrow=2),log=TRUE))}

# Find the optimal parameters using nlimnb:
opt=nlminb(c(0.01),Normal.likelihood,lower=c(0),upper=c(Inf), dat=dat)
# Find the Fisher information:
H=hessian(Normal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))

CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
#CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)

return(list("opt"=opt,"CI1"=CI1,"se"=se, "H"=H))
}

mod3<-normal.function(e)
mod3
```

#### Likelihood ratio test
```{r}
c = mod2$opt$objective-mod3$opt$objective
pchisq(-2*c, df=1,lower.tail = FALSE)
```
The p-value is extremely low, meaning that the null hypothesis is rejected. There is a difference between the two models.

#### Wald test
```{r}
# parameter delt med se
z=mod2$opt$par[2]/mod2$se[2]
2*(1-pnorm(z))
```
The p-value is extremely low, meaning that the null hypothesis is rejected. There is a difference between having the $\rho$ parameter and not.

### 3. Compare the Information matrix calculated by numerical methods with the algebraric form for the Fisher information I(sigmaˆ2,rho).
Compare with the algebraric:
```{r}
rho<-mod2$opt$par[[2]]
n<-dim(e)[1]
sigma2 <- mod2$opt$par[[1]]
H=mod2$H
  
I<-matrix(NA, nrow=2, ncol=2)
I[1,1]=n/sigma2^2
I[1,2]=-n*rho/(sigma2*(1-rho^2))
I[2,1]=-n*rho/(sigma2*(1-rho^2))
I[2,2]=(n*(1+rho^2))/(1-rho^2)^2

(H-I)/H

```
The difference between the calculated information matrix and the algebraric form for the fisher information is very small. They are more or less the same.


### 4. Make a plot of the profile liklihood ofρand compare with the quadraticapproximation.  Further using the z-transform (see Exercise 3.23) for ρ and the log-transform for σ2, plot the profile likelihood for z and compare with the quadratic approximation, finally derive Fishers in-formation matrix for the transformed varaibles and compare with thenumerical Hessian

```{r}


## Plot profile likelihood
rho <- seq(0,0.999, length=200)
## Profile likelihood for rho
lp.rho <- function(rho1, dat){
  
  #
  
  # Define likelihood function:
  Normal.likelihood = function(params,dat){ -sum(dmvnorm(dat, mean=c(0,0),sigma=params[1]*matrix(c(1,params[2],params[2],1), nrow=2),log=TRUE))}
  
  fun.tmp <- function(sigma, rho1, dat){
  -Normal.likelihood(c(sigma,rho1),dat) 
  }
  ## interval from plot
  optimize(fun.tmp,c(0.001,100),rho1=rho1,
           dat=dat,maximum=TRUE)$objective
}


Normal.likelihood = function(params,dat){ -sum(dmvnorm(dat, mean=c(0,0),sigma=params[1]*matrix(c(1,params[2],params[2],1), nrow=2),log=TRUE))}


```
```{r}

optmvt = mod2$opt
par(mfrow=c(1,2))
llp <- sapply(rho, lp.rho, dat=e)

plot(rho,llp-max(llp),type="l", main = "Profile log-likelihood for rho", xlab = expression(rho), ylab="Likelihood", ylim=c(-10,0),xlim = c(0,0.8))

lines(optmvt$par[2]*c(1,1),c(0,-20),col="blue")
pll = function(theta,dat){-Normal.likelihood(theta,dat)}
obs.info = hessian(pll,optmvt$par,dat=e)
ql = 0.5* obs.info[2,2] * (rho-optmvt$par[2])^2
lines(rho, ql,lty=2, col="red")

plot(rho,exp(llp-max(llp)),type="l", main = "Normalized profile likelihood for rho", xlab = expression(rho), ylab="Likelihood")
lines(optmvt$par[2]*c(1,1),c(0,1),col="blue",ylim = c(0,1), xlim = c(0,0.8))
lines(range(rho),
      exp(-c(1,1) * qchisq(0.95,df=1)/2),
      col=6,lty=2)
lines(rho, exp(ql-max(ql)),lty=2, col="red")



```

It can be concluded that the quadratic approximation is a good approximation of the likelihood


```{r}
sig1 = seq(10,20,length.out = 100)


z = 1/2*log((1+rho)/(1-rho))
log_s = log(sig1)
llmvt.trans = function(theta,e){
  # theta = log(sigma), z
  sigma = exp(theta[1])
  rho = (exp(2*theta[2]) - 1)/(exp(2*theta[2]) + 1)
  Normal.likelihood(c(sigma,rho),e)
}
lp.z <- function(z, x){
  #rho = (exp(2*z) - 1)/(exp(2*z) + 1)
  fun.tmp <- function(log_s, z, x){
    #sigma = exp(log_s)
    -llmvt.trans(c(log_s,z),x)
  }
  ## interval from plot
  optimize(fun.tmp,c(-1000,1000),z=z,
           x=x,maximum=TRUE)$objective
}
opt.trans = nlminb(c(0,0),llmvt.trans,e=e)
llp <- sapply(z,lp.z,x=e)
par(mfrow=c(1,2))
plot(rho,(llp-max(llp)),type="l", main = "Profile likelihood for z", xlab = expression(rho), ylab="Likelihood", ylim=c(-10,0))
pll.trans = function(theta,e){-llmvt.trans(theta,e)}
obs.info = hessian(pll.trans,opt.trans$par,e=e)
ql = 0.5* obs.info[2,2] * (z-opt.trans$par[2])^2
lines(rho, ql,lty=2, col="red")
lines(optmvt$par[2]*c(1,1),c(0,-20),col="blue")

plot(rho,exp(llp-max(llp)),type="l", main = "Profile likelihood for z", xlab = expression(rho), ylab="Likelihood")
lines(optmvt$par[2]*c(1,1),c(0,1),col="blue")
lines(range(z), exp(-c(1,1) * qchisq(0.95,df=1)/2), col=2,lty=2)
lines(rho, exp(ql-max(ql)),lty=2, col="red")





```

Optimal likelihood findes for transformeringen
Vi optimerer likelihood for de transformerede parametre.
Det ses at det gør en minimal forskel at bruge de transformerede parametre.


### 5. Estimate the parameters of the AR(1) model (see Example 11.1), first conditioning on e1, then full estimation. Compare with the estimation above, is it as expected?

```{r}

e <- residuals(mod0)
nll.ar1 <- function(pars,e){ 
  n <- length(e)
  sigma <- pars[1]
  #sigma <- exp(pars[1])
  rho <- (exp(2*pars[2])-1)/(1+exp(2*pars[2]))
  - sum(dnorm(e[-1], mean = rho*e[-n], sd = sqrt(sigma), log = TRUE))
}

pars <- c(2,0.2)
opt.ar1 <- nlminb(pars,nll.ar1,e=e,lower=c(0,-Inf),upper=c(Inf,Inf))
#opt.ar1 <- nlminb(pars,nll.ar1,e=e,lower=c(-Inf,-Inf),upper=c(Inf,Inf))
(opt.ar1)

```



```{r}

e <- residuals(mod0)
nll.ar1 <- function(pars,e){ 
  n <- length(e)
  sigma <- pars[1]
  #sigma <- exp(pars[1])
  rho <- (exp(2*pars[2])-1)/(1+exp(2*pars[2]))
  sigma0 <- sigma / (1-rho^2)
  -dnorm(e[1], sd = sqrt(sigma0), log = TRUE)-sum(dnorm(e[-1], mean = rho*e[-n], sd = sqrt(sigma), log = TRUE))
}

pars <- c(2,0.2)
#opt.ar1 <- nlminb(pars,nll.ar1,e=e,lower=c(-Inf,-Inf),upper=c(Inf,Inf))
opt.ar1.full <- nlminb(pars,nll.ar1,e=e,lower=c(0,-Inf),upper=c(Inf,Inf))
opt.ar1.full
```

### 6. Estimate the parameters of the linear model (2) and the parameters of the AR(1) model simultanious, and compare the likelihood of the linear model and the combined model.

```{r}

e <- cbind(mod0$residuals[1:length(mod0$residuals)-1],mod0$residuals[2:length(mod0$residuals)])
#e <- residuals(mod0)
nll.ar1.full <- function(pars,y,X){
  n <- length(y)
  y.hat <- X%*%pars[1:dim(X)[2]] 
  pars <- pars[-(1:dim(X)[2])] 
  e<-y-y.hat
  sigma <- exp(pars[1])
  rho <- (exp(2*pars[2])-1)/(1+exp(2*pars[2]))
  sigma0 <- sigma / (1-rho^2)
  - dnorm(e[1], sd = sqrt(sigma0), log = TRUE) - sum(dnorm(e[-1], mean = rho*e[-n], sd=sqrt(sigma), log = TRUE))
}

params <- c(coef(mod0),opt.ar1.full$par)


X <-model.matrix(mod0)

(opt <- nlminb(c(params)*0, nll.ar1.full, y=trans1(opt.lambda.trans1$minimum,data$pow.obs.norm), X=X))

```

```{r}
rbind(coef(mod0), opt$par[1:4])
```

Coeffcienterne i den lineære model ændre sig ikke særlig meget, derfor er antagelsen om at residualerne følger en AR(1) passer.

### 7. Discuss the effect of including the AR(1)-term for short and long term preddictions. 

```{r}
acf(mod0$residuals, main = "Autocorrelation of linear model")
```
```{r}
error=rep(NA, (n-1))
for( i in 1:(n-1)){
  y = e[i,2]*opt.ar1.full$par[2]+rnorm(1, mean=0, sd=opt.ar1.full$par[2])
  error[i]= e[i,1]-y
}

acf(error, main = "Autocorrelation of AR(1) model")
```






