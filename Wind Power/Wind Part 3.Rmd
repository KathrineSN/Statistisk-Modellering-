---
title: "Wind Part 3"
output: pdf_document
---

## Read the data tuno.txt into R.
The data is read, and the first few lines of the code is shown.
```{r}
data <- read.delim("tuno.txt",header = TRUE, sep = " ", dec = ".")
data$pow.obs.norm<-(data$pow.obs)/5000
head(data,6)
```
```{r}
plot(data$r.day,data$pow.obs.norm,type='o')
```

## Fit optimal linear model

```{r}
trans1 <- function(lambda,y){
    y.l <- 1/lambda*log((y^lambda)/(1-y^lambda))
    return(y.l)}

## profile likelihood for lambda
lp.lambda1 <- function(lambda,y){
    mod <- lm(trans1(lambda,y)~data$ws30+I(data$ws30^2)+cos(data$wd30))
    length(y)/2*log(summary(mod)$sigma^2) - sum(log(abs(1/(y*(1 - y^lambda)))))
    }

(opt.lambda.trans1=optimize(lp.lambda1,c(0,1),y=data$pow.obs.norm))

mod0=glm(trans1(opt.lambda.trans1$minimum,data$pow.obs.norm)~data$ws30+I(data$ws30^2)+cos(data$wd30),family=gaussian,  data=data)

mod0
```


## 1. Extract residuals and construct matrix(e)
```{r}

e <- cbind(mod0$residuals[1:length(mod0$residuals)-1],mod0$residuals[2:length(mod0$residuals)])

# e <- cbind(mod0$residuals[2:length(mod0$residuals)],mod0$residuals[1:length(mod0$residuals)-1])

var(e);cor(e)

```

## 2. Fit e to a normal distribution

Skal det her ikke flyttes? Har ikke så meget at gøre med at fitte en normalfordeing :)
```{r}
mod1 <- arima(mod0$residuals ,order=c(1,0,0),include.mean=F)
par(mfrow=c(2,1))
acf(mod1$residuals,lag.max = 50)
pacf(mod1$residuals,lag.max=50)
```

### Parameter estimates and wald confidence intervals
For model where $\rho$ is not equal to zero.
```{r}
library(numDeriv)
library(mvtnorm)
normal.function=function(dat){
# Define likelihood function:
Normal.likelihood=function(params,dat){ -sum(dmvnorm(e, mean=c(0,0),sigma=params[1]*matrix(c(1,params[2], params[2],1), nrow=2),log=TRUE))}

# Find the optimal parameters using nlimnb:
opt=nlminb(c(0.01,0.01),Normal.likelihood,lower=c(0,-1),upper=c(Inf, 1), dat=dat)
# Find the Fisher information:
H=hessian(Normal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))

CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)

return(list("opt"=opt,"CI1"=CI1,"CI2"=CI2, "se"=se, "H"=H))
}

mod2<-normal.function(e)
mod2
```



### A contour plot of the likelihood

```{r}

sig1 = seq(9,15,length.out = 100)
rho1 = seq(0,0.7, length.out = 100)
par(mfrow=c(1,1))
ll.contour = function(sigma,rho, e){
  n <- length(e) - 1
  #print(co)
  tmp=0
  deter = sigma^2-rho^2*sigma^2
  for (i in 1:n){
    x=e[i]; y=e[i+1]
    tmp = tmp + 1/2*( (-x/( sigma*(rho^2 - 1)) + y*rho/( sigma*(rho^2 - 1)))*x 
                                               + (x*rho/( sigma*(rho^2 - 1)) - y/( sigma*(rho^2 - 1)))*y)
  }
  -2/2*log(2*pi)*n-1/2*log(deter)*n - tmp #1/(2*sigma) * sum((y[-1]-co*y[-(n+1)])^2)
}
ll<- outer(sig1,rho1,'ll.contour',e=e)
like<- exp(ll-max(ll))
contour(sig1,rho1,like,level=c(0.05,.1,.3,.5,.7,.9),
        ylab=expression(rho),
        xlab=expression(sigma^2))
title(expression('Likelihood contour'))
lines(c(0,50),c(0,0),lty=2,col=2)

```


### P-values for the likelihood ratio test
Null model, where $\rho=0$.

```{r}
normal.function=function(dat){
# Define likelihood function:
Normal.likelihood=function(params,dat){ -sum(dmvnorm(e, mean=c(0,0),sigma=params[1]*matrix(c(1,0,0,1), nrow=2),log=TRUE))}

# Find the optimal parameters using nlimnb:
opt=nlminb(c(0.01),Normal.likelihood,lower=c(0),upper=c(Inf), dat=dat)
# Find the Fisher information:
H=hessian(Normal.likelihood,opt$par, dat=dat)
se=sqrt(diag(solve(H)))

CI1=opt$par[1]+c(-1,1)*se[1]*qnorm(0.975)
#CI2=opt$par[2]+c(-1,1)*se[2]*qnorm(0.975)

return(list("opt"=opt,"CI1"=CI1,"se"=se, "H"=H))
}

mod3<-normal.function(e)
mod3
```

#### Likelihood ratio test
```{r}
c = mod2$opt$objective-mod3$opt$objective
pchisq(-2*c, df=1,lower.tail = FALSE)
```
The p-value is extremely low, meaning that the null hypothesis is rejected. There is a difference between the two models.

#### Wald test
```{r}
# parameter delt med se
z=mod2$opt$par[2]/mod2$se[2]
2*(1-pnorm(z))
```
The p-value is extremely low, meaning that the null hypothesis is rejected. There is a difference between having the $\rho$ parameter and not.

### 3. Compare the Information matrix calculated by numerical methods with the algebraric form for the Fisher information I(sigmaˆ2,rho).
Compare with the algebraric:
```{r}
rho<-mod2$opt$par[[2]]
n<-dim(e)[1]
sigma2 <- mod2$opt$par[[1]]
H=mod2$H
  
I<-matrix(NA, nrow=2, ncol=2)
I[1,1]=n/sigma2^2
I[1,2]=-n*rho/(sigma2*(1-rho^2))
I[2,1]=-n*rho/(sigma2*(1-rho^2))
I[2,2]=(n*(1+rho^2))/(1-rho^2)^2

(H-I)/H

```
The difference between the calculated information matrix and the algebraric form for the fisher information is very small. They are more or less the same.


### 4. 

```{r}

```




### 5. Estimate the parameters of the AR(1) model (see Example 11.1), first conditioning on e1, then full estimation. Compare with the estimation above, is it as expected?

```{r}

e <- residuals(mod0)
nll.ar1 <- function(pars,e){ 
  n <- length(e)
  sigma <- pars[1]
  #sigma <- exp(pars[1])
  rho <- (exp(2*pars[2])-1)/(1+exp(2*pars[2]))
  - sum(dnorm(e[-1], mean = rho*e[-n], sd = sqrt(sigma), log = TRUE))
}

pars <- c(2,0.2)
opt.ar1 <- nlminb(pars,nll.ar1,e=e,lower=c(0,-Inf),upper=c(Inf,Inf))
#opt.ar1 <- nlminb(pars,nll.ar1,e=e,lower=c(-Inf,-Inf),upper=c(Inf,Inf))
(opt.ar1)

```



```{r}

e <- residuals(mod0)
nll.ar1 <- function(pars,e){ 
  n <- length(e)
  sigma <- pars[1]
  #sigma <- exp(pars[1])
  rho <- (exp(2*pars[2])-1)/(1+exp(2*pars[2]))
  sigma0 <- sigma / (1-rho^2)
  -dnorm(e[1], sd = sqrt(sigma0), log = TRUE)-sum(dnorm(e[-1], mean = rho*e[-n], sd = sqrt(sigma), log = TRUE))
}

pars <- c(2,0.2)
#opt.ar1 <- nlminb(pars,nll.ar1,e=e,lower=c(-Inf,-Inf),upper=c(Inf,Inf))
opt.ar1 <- nlminb(pars,nll.ar1,e=e,lower=c(0,-Inf),upper=c(Inf,Inf))
(opt.ar1)

```

### 6. Estimate the parameters of the linear model (2) and the parameters of the AR(1) model simultanious, and compare the likelihood of the linear model and the combined model.

```{r}

X=cbind(rep(1,length(data$ws30)),data$ws30,data$ws30^2, cos(data$wd30))
nll.ar1.full <- function(pars,y,X){
  n <- length(y)
  y.hat <- X%*%pars[1:dim(X)[2]] 
  pars <- pars[-(1:dim(X)[2])] 
  e<-y-y.hat
  sigma <- exp(pars[1])
  rho <- (exp(2*pars[2])-1)/(1+exp(2*pars[2]))
  sigma0 <- sigma / (1-rho^2)
  - dnorm(e[1], sd = sqrt(sigma0), log = TRUE) - sum(dnorm(e[-1]), mean = rho*e[-n], sd=sqrt(sigma), log = TRUE)
}

pars <- c(0.01,0.01,0.01,0.01,0.01,0.01)

(opt.ar1.full <- nlminb(c(log(pars[1]),pars[2],pars[3],pars[4]), nll.ar1.full, X=X, y=data$pow.obs.norm,lower = c(-Inf,-Inf,-Inf,-Inf), upper=c(Inf,Inf,Inf,Inf)))

```

```{r}
X=cbind(rep(1,length(data$ws30)),data$ws30,data$ws30^2, cos(data$wd30))
dim(X)
```







